{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary image processing task\n",
    "* CUDA, cuDNN, and cuBLAS libraries support for GPU acceleration - Yet to be done. \n",
    "* Pre-trained Convolutional Neural Networks (CNNs) such as VGG, MobileNet, DenseNet, ResNet, InceptionV4, and CoCa\n",
    "* Dropout layers with a dropout rate of 0.2 to prevent overfitting\n",
    "* Reduce batch sizes from 64 or 32 if memory limitations\n",
    "* train each model for 30 epochs initially, but adjust the number of epochs based on the observed accuracy during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hifia\\anaconda3\\envs\\DementiaGPU\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bat_resnext26ts.ch_in1k', 'beit_base_patch16_224.in22k_ft_in22k', 'beit_base_patch16_224.in22k_ft_in22k_in1k', 'beit_base_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_224.in22k_ft_in22k', 'beit_large_patch16_224.in22k_ft_in22k_in1k', 'beit_large_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_512.in22k_ft_in22k_in1k', 'beitv2_base_patch16_224.in1k_ft_in1k', 'beitv2_base_patch16_224.in1k_ft_in22k', 'beitv2_base_patch16_224.in1k_ft_in22k_in1k', 'beitv2_large_patch16_224.in1k_ft_in1k', 'beitv2_large_patch16_224.in1k_ft_in22k', 'beitv2_large_patch16_224.in1k_ft_in22k_in1k', 'botnet26t_256.c1_in1k', 'caformer_b36.sail_in1k', 'caformer_b36.sail_in1k_384', 'caformer_b36.sail_in22k', 'caformer_b36.sail_in22k_ft_in1k', 'caformer_b36.sail_in22k_ft_in1k_384', 'caformer_m36.sail_in1k', 'caformer_m36.sail_in1k_384', 'caformer_m36.sail_in22k', 'caformer_m36.sail_in22k_ft_in1k', 'caformer_m36.sail_in22k_ft_in1k_384', 'caformer_s18.sail_in1k', 'caformer_s18.sail_in1k_384', 'caformer_s18.sail_in22k', 'caformer_s18.sail_in22k_ft_in1k', 'caformer_s18.sail_in22k_ft_in1k_384', 'caformer_s36.sail_in1k', 'caformer_s36.sail_in1k_384', 'caformer_s36.sail_in22k', 'caformer_s36.sail_in22k_ft_in1k', 'caformer_s36.sail_in22k_ft_in1k_384', 'cait_m36_384.fb_dist_in1k', 'cait_m48_448.fb_dist_in1k', 'cait_s24_224.fb_dist_in1k', 'cait_s24_384.fb_dist_in1k', 'cait_s36_384.fb_dist_in1k', 'cait_xs24_384.fb_dist_in1k', 'cait_xxs24_224.fb_dist_in1k', 'cait_xxs24_384.fb_dist_in1k', 'cait_xxs36_224.fb_dist_in1k', 'cait_xxs36_384.fb_dist_in1k', 'coat_lite_medium.in1k', 'coat_lite_medium_384.in1k', 'coat_lite_mini.in1k', 'coat_lite_small.in1k', 'coat_lite_tiny.in1k', 'coat_mini.in1k', 'coat_small.in1k', 'coat_tiny.in1k', 'coatnet_0_rw_224.sw_in1k', 'coatnet_1_rw_224.sw_in1k', 'coatnet_2_rw_224.sw_in12k', 'coatnet_2_rw_224.sw_in12k_ft_in1k', 'coatnet_3_rw_224.sw_in12k', 'coatnet_bn_0_rw_224.sw_in1k', 'coatnet_nano_rw_224.sw_in1k', 'coatnet_rmlp_1_rw2_224.sw_in12k', 'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k', 'coatnet_rmlp_1_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in12k', 'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k', 'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k', 'coatnet_rmlp_nano_rw_224.sw_in1k', 'coatnext_nano_rw_224.sw_in1k', 'convformer_b36.sail_in1k', 'convformer_b36.sail_in1k_384', 'convformer_b36.sail_in22k', 'convformer_b36.sail_in22k_ft_in1k', 'convformer_b36.sail_in22k_ft_in1k_384', 'convformer_m36.sail_in1k', 'convformer_m36.sail_in1k_384', 'convformer_m36.sail_in22k', 'convformer_m36.sail_in22k_ft_in1k', 'convformer_m36.sail_in22k_ft_in1k_384', 'convformer_s18.sail_in1k', 'convformer_s18.sail_in1k_384', 'convformer_s18.sail_in22k', 'convformer_s18.sail_in22k_ft_in1k', 'convformer_s18.sail_in22k_ft_in1k_384', 'convformer_s36.sail_in1k', 'convformer_s36.sail_in1k_384', 'convformer_s36.sail_in22k', 'convformer_s36.sail_in22k_ft_in1k', 'convformer_s36.sail_in22k_ft_in1k_384', 'convit_base.fb_in1k', 'convit_small.fb_in1k', 'convit_tiny.fb_in1k', 'convmixer_768_32.in1k', 'convmixer_1024_20_ks9_p14.in1k', 'convmixer_1536_20.in1k', 'convnext_atto.d2_in1k', 'convnext_atto_ols.a2_in1k', 'convnext_base.clip_laion2b', 'convnext_base.clip_laion2b_augreg', 'convnext_base.clip_laion2b_augreg_ft_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384', 'convnext_base.clip_laiona', 'convnext_base.clip_laiona_320', 'convnext_base.clip_laiona_augreg_320', 'convnext_base.clip_laiona_augreg_ft_in1k_384', 'convnext_base.fb_in1k', 'convnext_base.fb_in22k', 'convnext_base.fb_in22k_ft_in1k', 'convnext_base.fb_in22k_ft_in1k_384', 'convnext_femto.d1_in1k', 'convnext_femto_ols.d1_in1k', 'convnext_large.fb_in1k', 'convnext_large.fb_in22k', 'convnext_large.fb_in22k_ft_in1k', 'convnext_large.fb_in22k_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_ft_320', 'convnext_large_mlp.clip_laion2b_ft_soup_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384', 'convnext_nano.d1h_in1k', 'convnext_nano.in12k', 'convnext_nano.in12k_ft_in1k', 'convnext_nano_ols.d1h_in1k', 'convnext_pico.d1_in1k', 'convnext_pico_ols.d1_in1k', 'convnext_small.fb_in1k', 'convnext_small.fb_in22k', 'convnext_small.fb_in22k_ft_in1k', 'convnext_small.fb_in22k_ft_in1k_384', 'convnext_small.in12k', 'convnext_small.in12k_ft_in1k', 'convnext_small.in12k_ft_in1k_384', 'convnext_tiny.fb_in1k', 'convnext_tiny.fb_in22k', 'convnext_tiny.fb_in22k_ft_in1k', 'convnext_tiny.fb_in22k_ft_in1k_384', 'convnext_tiny.in12k', 'convnext_tiny.in12k_ft_in1k', 'convnext_tiny.in12k_ft_in1k_384', 'convnext_tiny_hnf.a2h_in1k', 'convnext_xlarge.fb_in22k', 'convnext_xlarge.fb_in22k_ft_in1k', 'convnext_xlarge.fb_in22k_ft_in1k_384', 'convnext_xxlarge.clip_laion2b_rewind', 'convnext_xxlarge.clip_laion2b_soup', 'convnext_xxlarge.clip_laion2b_soup_ft_in1k', 'convnextv2_atto.fcmae', 'convnextv2_atto.fcmae_ft_in1k', 'convnextv2_base.fcmae', 'convnextv2_base.fcmae_ft_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k_384', 'convnextv2_femto.fcmae', 'convnextv2_femto.fcmae_ft_in1k', 'convnextv2_huge.fcmae', 'convnextv2_huge.fcmae_ft_in1k', 'convnextv2_huge.fcmae_ft_in22k_in1k_384', 'convnextv2_huge.fcmae_ft_in22k_in1k_512', 'convnextv2_large.fcmae', 'convnextv2_large.fcmae_ft_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k_384', 'convnextv2_nano.fcmae', 'convnextv2_nano.fcmae_ft_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k_384', 'convnextv2_pico.fcmae', 'convnextv2_pico.fcmae_ft_in1k', 'convnextv2_tiny.fcmae', 'convnextv2_tiny.fcmae_ft_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k_384', 'crossvit_9_240.in1k', 'crossvit_9_dagger_240.in1k', 'crossvit_15_240.in1k', 'crossvit_15_dagger_240.in1k', 'crossvit_15_dagger_408.in1k', 'crossvit_18_240.in1k', 'crossvit_18_dagger_240.in1k', 'crossvit_18_dagger_408.in1k', 'crossvit_base_240.in1k', 'crossvit_small_240.in1k', 'crossvit_tiny_240.in1k', 'cs3darknet_focus_l.c2ns_in1k', 'cs3darknet_focus_m.c2ns_in1k', 'cs3darknet_l.c2ns_in1k', 'cs3darknet_m.c2ns_in1k', 'cs3darknet_x.c2ns_in1k', 'cs3edgenet_x.c2_in1k', 'cs3se_edgenet_x.c2ns_in1k', 'cs3sedarknet_l.c2ns_in1k', 'cs3sedarknet_x.c2ns_in1k', 'cspdarknet53.ra_in1k', 'cspresnet50.ra_in1k', 'cspresnext50.ra_in1k', 'darknet53.c2ns_in1k', 'darknetaa53.c2ns_in1k', 'davit_base.msft_in1k', 'davit_small.msft_in1k', 'davit_tiny.msft_in1k', 'deit3_base_patch16_224.fb_in1k', 'deit3_base_patch16_224.fb_in22k_ft_in1k', 'deit3_base_patch16_384.fb_in1k', 'deit3_base_patch16_384.fb_in22k_ft_in1k', 'deit3_huge_patch14_224.fb_in1k', 'deit3_huge_patch14_224.fb_in22k_ft_in1k', 'deit3_large_patch16_224.fb_in1k', 'deit3_large_patch16_224.fb_in22k_ft_in1k', 'deit3_large_patch16_384.fb_in1k', 'deit3_large_patch16_384.fb_in22k_ft_in1k', 'deit3_medium_patch16_224.fb_in1k', 'deit3_medium_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_224.fb_in1k', 'deit3_small_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_384.fb_in1k', 'deit3_small_patch16_384.fb_in22k_ft_in1k', 'deit_base_distilled_patch16_224.fb_in1k', 'deit_base_distilled_patch16_384.fb_in1k', 'deit_base_patch16_224.fb_in1k', 'deit_base_patch16_384.fb_in1k', 'deit_small_distilled_patch16_224.fb_in1k', 'deit_small_patch16_224.fb_in1k', 'deit_tiny_distilled_patch16_224.fb_in1k', 'deit_tiny_patch16_224.fb_in1k', 'densenet121.ra_in1k', 'densenet121.tv_in1k', 'densenet161.tv_in1k', 'densenet169.tv_in1k', 'densenet201.tv_in1k', 'densenetblur121d.ra_in1k', 'dla34.in1k', 'dla46_c.in1k', 'dla46x_c.in1k', 'dla60.in1k', 'dla60_res2net.in1k', 'dla60_res2next.in1k', 'dla60x.in1k', 'dla60x_c.in1k', 'dla102.in1k', 'dla102x2.in1k', 'dla102x.in1k', 'dla169.in1k', 'dm_nfnet_f0.dm_in1k', 'dm_nfnet_f1.dm_in1k', 'dm_nfnet_f2.dm_in1k', 'dm_nfnet_f3.dm_in1k', 'dm_nfnet_f4.dm_in1k', 'dm_nfnet_f5.dm_in1k', 'dm_nfnet_f6.dm_in1k', 'dpn68.mx_in1k', 'dpn68b.mx_in1k', 'dpn68b.ra_in1k', 'dpn92.mx_in1k', 'dpn98.mx_in1k', 'dpn107.mx_in1k', 'dpn131.mx_in1k', 'eca_botnext26ts_256.c1_in1k', 'eca_halonext26ts.c1_in1k', 'eca_nfnet_l0.ra2_in1k', 'eca_nfnet_l1.ra2_in1k', 'eca_nfnet_l2.ra3_in1k', 'eca_resnet33ts.ra2_in1k', 'eca_resnext26ts.ch_in1k', 'ecaresnet26t.ra2_in1k', 'ecaresnet50d.miil_in1k', 'ecaresnet50d_pruned.miil_in1k', 'ecaresnet50t.a1_in1k', 'ecaresnet50t.a2_in1k', 'ecaresnet50t.a3_in1k', 'ecaresnet50t.ra2_in1k', 'ecaresnet101d.miil_in1k', 'ecaresnet101d_pruned.miil_in1k', 'ecaresnet269d.ra2_in1k', 'ecaresnetlight.miil_in1k', 'edgenext_base.in21k_ft_in1k', 'edgenext_base.usi_in1k', 'edgenext_small.usi_in1k', 'edgenext_small_rw.sw_in1k', 'edgenext_x_small.in1k', 'edgenext_xx_small.in1k', 'efficientformer_l1.snap_dist_in1k', 'efficientformer_l3.snap_dist_in1k', 'efficientformer_l7.snap_dist_in1k', 'efficientformerv2_l.snap_dist_in1k', 'efficientformerv2_s0.snap_dist_in1k', 'efficientformerv2_s1.snap_dist_in1k', 'efficientformerv2_s2.snap_dist_in1k', 'efficientnet_b0.ra_in1k', 'efficientnet_b1.ft_in1k', 'efficientnet_b1_pruned.in1k', 'efficientnet_b2.ra_in1k', 'efficientnet_b2_pruned.in1k', 'efficientnet_b3.ra2_in1k', 'efficientnet_b3_pruned.in1k', 'efficientnet_b4.ra2_in1k', 'efficientnet_b5.sw_in12k', 'efficientnet_b5.sw_in12k_ft_in1k', 'efficientnet_el.ra_in1k', 'efficientnet_el_pruned.in1k', 'efficientnet_em.ra2_in1k', 'efficientnet_es.ra_in1k', 'efficientnet_es_pruned.in1k', 'efficientnet_lite0.ra_in1k', 'efficientnetv2_rw_m.agc_in1k', 'efficientnetv2_rw_s.ra2_in1k', 'efficientnetv2_rw_t.ra2_in1k', 'efficientvit_b0.r224_in1k', 'efficientvit_b1.r224_in1k', 'efficientvit_b1.r256_in1k', 'efficientvit_b1.r288_in1k', 'efficientvit_b2.r224_in1k', 'efficientvit_b2.r256_in1k', 'efficientvit_b2.r288_in1k', 'efficientvit_b3.r224_in1k', 'efficientvit_b3.r256_in1k', 'efficientvit_b3.r288_in1k', 'efficientvit_m0.r224_in1k', 'efficientvit_m1.r224_in1k', 'efficientvit_m2.r224_in1k', 'efficientvit_m3.r224_in1k', 'efficientvit_m4.r224_in1k', 'efficientvit_m5.r224_in1k', 'ese_vovnet19b_dw.ra_in1k', 'ese_vovnet39b.ra_in1k', 'eva02_base_patch14_224.mim_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in1k', 'eva02_base_patch14_448.mim_in22k_ft_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_base_patch16_clip_224.merged2b', 'eva02_enormous_patch14_clip_224.laion2b', 'eva02_enormous_patch14_clip_224.laion2b_plus', 'eva02_large_patch14_224.mim_in22k', 'eva02_large_patch14_224.mim_m38m', 'eva02_large_patch14_448.mim_in22k_ft_in1k', 'eva02_large_patch14_448.mim_in22k_ft_in22k', 'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in22k', 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', 'eva02_large_patch14_clip_224.merged2b', 'eva02_large_patch14_clip_336.merged2b', 'eva02_small_patch14_224.mim_in22k', 'eva02_small_patch14_336.mim_in22k_ft_in1k', 'eva02_tiny_patch14_224.mim_in22k', 'eva02_tiny_patch14_336.mim_in22k_ft_in1k', 'eva_giant_patch14_224.clip_ft_in1k', 'eva_giant_patch14_336.clip_ft_in1k', 'eva_giant_patch14_336.m30m_ft_in22k_in1k', 'eva_giant_patch14_560.m30m_ft_in22k_in1k', 'eva_giant_patch14_clip_224.laion400m', 'eva_giant_patch14_clip_224.merged2b', 'eva_large_patch14_196.in22k_ft_in1k', 'eva_large_patch14_196.in22k_ft_in22k_in1k', 'eva_large_patch14_336.in22k_ft_in1k', 'eva_large_patch14_336.in22k_ft_in22k_in1k', 'fastvit_ma36.apple_dist_in1k', 'fastvit_ma36.apple_in1k', 'fastvit_s12.apple_dist_in1k', 'fastvit_s12.apple_in1k', 'fastvit_sa12.apple_dist_in1k', 'fastvit_sa12.apple_in1k', 'fastvit_sa24.apple_dist_in1k', 'fastvit_sa24.apple_in1k', 'fastvit_sa36.apple_dist_in1k', 'fastvit_sa36.apple_in1k', 'fastvit_t8.apple_dist_in1k', 'fastvit_t8.apple_in1k', 'fastvit_t12.apple_dist_in1k', 'fastvit_t12.apple_in1k', 'fbnetc_100.rmsp_in1k', 'fbnetv3_b.ra2_in1k', 'fbnetv3_d.ra2_in1k', 'fbnetv3_g.ra2_in1k', 'flexivit_base.300ep_in1k', 'flexivit_base.300ep_in21k', 'flexivit_base.600ep_in1k', 'flexivit_base.1000ep_in21k', 'flexivit_base.1200ep_in1k', 'flexivit_base.patch16_in21k', 'flexivit_base.patch30_in21k', 'flexivit_large.300ep_in1k', 'flexivit_large.600ep_in1k', 'flexivit_large.1200ep_in1k', 'flexivit_small.300ep_in1k', 'flexivit_small.600ep_in1k', 'flexivit_small.1200ep_in1k', 'focalnet_base_lrf.ms_in1k', 'focalnet_base_srf.ms_in1k', 'focalnet_huge_fl3.ms_in22k', 'focalnet_huge_fl4.ms_in22k', 'focalnet_large_fl3.ms_in22k', 'focalnet_large_fl4.ms_in22k', 'focalnet_small_lrf.ms_in1k', 'focalnet_small_srf.ms_in1k', 'focalnet_tiny_lrf.ms_in1k', 'focalnet_tiny_srf.ms_in1k', 'focalnet_xlarge_fl3.ms_in22k', 'focalnet_xlarge_fl4.ms_in22k', 'gc_efficientnetv2_rw_t.agc_in1k', 'gcresnet33ts.ra2_in1k', 'gcresnet50t.ra2_in1k', 'gcresnext26ts.ch_in1k', 'gcresnext50ts.ch_in1k', 'gcvit_base.in1k', 'gcvit_small.in1k', 'gcvit_tiny.in1k', 'gcvit_xtiny.in1k', 'gcvit_xxtiny.in1k', 'gernet_l.idstcv_in1k', 'gernet_m.idstcv_in1k', 'gernet_s.idstcv_in1k', 'ghostnet_100.in1k', 'ghostnetv2_100.in1k', 'ghostnetv2_130.in1k', 'ghostnetv2_160.in1k', 'gmixer_24_224.ra3_in1k', 'gmlp_s16_224.ra3_in1k', 'halo2botnet50ts_256.a1h_in1k', 'halonet26t.a1h_in1k', 'halonet50ts.a1h_in1k', 'haloregnetz_b.ra3_in1k', 'hardcorenas_a.miil_green_in1k', 'hardcorenas_b.miil_green_in1k', 'hardcorenas_c.miil_green_in1k', 'hardcorenas_d.miil_green_in1k', 'hardcorenas_e.miil_green_in1k', 'hardcorenas_f.miil_green_in1k', 'hrnet_w18.ms_aug_in1k', 'hrnet_w18.ms_in1k', 'hrnet_w18_small.gluon_in1k', 'hrnet_w18_small.ms_in1k', 'hrnet_w18_small_v2.gluon_in1k', 'hrnet_w18_small_v2.ms_in1k', 'hrnet_w18_ssld.paddle_in1k', 'hrnet_w30.ms_in1k', 'hrnet_w32.ms_in1k', 'hrnet_w40.ms_in1k', 'hrnet_w44.ms_in1k', 'hrnet_w48.ms_in1k', 'hrnet_w48_ssld.paddle_in1k', 'hrnet_w64.ms_in1k', 'inception_next_base.sail_in1k', 'inception_next_base.sail_in1k_384', 'inception_next_small.sail_in1k', 'inception_next_tiny.sail_in1k', 'inception_resnet_v2.tf_ens_adv_in1k', 'inception_resnet_v2.tf_in1k', 'inception_v3.gluon_in1k', 'inception_v3.tf_adv_in1k', 'inception_v3.tf_in1k', 'inception_v3.tv_in1k', 'inception_v4.tf_in1k', 'lambda_resnet26rpt_256.c1_in1k', 'lambda_resnet26t.c1_in1k', 'lambda_resnet50ts.a1h_in1k', 'lamhalobotnet50ts_256.a1h_in1k', 'lcnet_050.ra2_in1k', 'lcnet_075.ra2_in1k', 'lcnet_100.ra2_in1k', 'legacy_senet154.in1k', 'legacy_seresnet18.in1k', 'legacy_seresnet34.in1k', 'legacy_seresnet50.in1k', 'legacy_seresnet101.in1k', 'legacy_seresnet152.in1k', 'legacy_seresnext26_32x4d.in1k', 'legacy_seresnext50_32x4d.in1k', 'legacy_seresnext101_32x4d.in1k', 'legacy_xception.tf_in1k', 'levit_128.fb_dist_in1k', 'levit_128s.fb_dist_in1k', 'levit_192.fb_dist_in1k', 'levit_256.fb_dist_in1k', 'levit_384.fb_dist_in1k', 'levit_conv_128.fb_dist_in1k', 'levit_conv_128s.fb_dist_in1k', 'levit_conv_192.fb_dist_in1k', 'levit_conv_256.fb_dist_in1k', 'levit_conv_384.fb_dist_in1k', 'maxvit_base_tf_224.in1k', 'maxvit_base_tf_224.in21k', 'maxvit_base_tf_384.in1k', 'maxvit_base_tf_384.in21k_ft_in1k', 'maxvit_base_tf_512.in1k', 'maxvit_base_tf_512.in21k_ft_in1k', 'maxvit_large_tf_224.in1k', 'maxvit_large_tf_224.in21k', 'maxvit_large_tf_384.in1k', 'maxvit_large_tf_384.in21k_ft_in1k', 'maxvit_large_tf_512.in1k', 'maxvit_large_tf_512.in21k_ft_in1k', 'maxvit_nano_rw_256.sw_in1k', 'maxvit_rmlp_base_rw_224.sw_in12k', 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k', 'maxvit_rmlp_nano_rw_256.sw_in1k', 'maxvit_rmlp_pico_rw_256.sw_in1k', 'maxvit_rmlp_small_rw_224.sw_in1k', 'maxvit_rmlp_tiny_rw_256.sw_in1k', 'maxvit_small_tf_224.in1k', 'maxvit_small_tf_384.in1k', 'maxvit_small_tf_512.in1k', 'maxvit_tiny_rw_224.sw_in1k', 'maxvit_tiny_tf_224.in1k', 'maxvit_tiny_tf_384.in1k', 'maxvit_tiny_tf_512.in1k', 'maxvit_xlarge_tf_224.in21k', 'maxvit_xlarge_tf_384.in21k_ft_in1k', 'maxvit_xlarge_tf_512.in21k_ft_in1k', 'maxxvit_rmlp_nano_rw_256.sw_in1k', 'maxxvit_rmlp_small_rw_256.sw_in1k', 'maxxvitv2_nano_rw_256.sw_in1k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k', 'mixer_b16_224.goog_in21k', 'mixer_b16_224.goog_in21k_ft_in1k', 'mixer_b16_224.miil_in21k', 'mixer_b16_224.miil_in21k_ft_in1k', 'mixer_l16_224.goog_in21k', 'mixer_l16_224.goog_in21k_ft_in1k', 'mixnet_l.ft_in1k', 'mixnet_m.ft_in1k', 'mixnet_s.ft_in1k', 'mixnet_xl.ra_in1k', 'mnasnet_100.rmsp_in1k', 'mnasnet_small.lamb_in1k', 'mobilenetv2_050.lamb_in1k', 'mobilenetv2_100.ra_in1k', 'mobilenetv2_110d.ra_in1k', 'mobilenetv2_120d.ra_in1k', 'mobilenetv2_140.ra_in1k', 'mobilenetv3_large_100.miil_in21k', 'mobilenetv3_large_100.miil_in21k_ft_in1k', 'mobilenetv3_large_100.ra_in1k', 'mobilenetv3_rw.rmsp_in1k', 'mobilenetv3_small_050.lamb_in1k', 'mobilenetv3_small_075.lamb_in1k', 'mobilenetv3_small_100.lamb_in1k', 'mobileone_s0.apple_in1k', 'mobileone_s1.apple_in1k', 'mobileone_s2.apple_in1k', 'mobileone_s3.apple_in1k', 'mobileone_s4.apple_in1k', 'mobilevit_s.cvnets_in1k', 'mobilevit_xs.cvnets_in1k', 'mobilevit_xxs.cvnets_in1k', 'mobilevitv2_050.cvnets_in1k', 'mobilevitv2_075.cvnets_in1k', 'mobilevitv2_100.cvnets_in1k', 'mobilevitv2_125.cvnets_in1k', 'mobilevitv2_150.cvnets_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k_384', 'mobilevitv2_175.cvnets_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k_384', 'mobilevitv2_200.cvnets_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k_384', 'mvitv2_base.fb_in1k', 'mvitv2_base_cls.fb_inw21k', 'mvitv2_huge_cls.fb_inw21k', 'mvitv2_large.fb_in1k', 'mvitv2_large_cls.fb_inw21k', 'mvitv2_small.fb_in1k', 'mvitv2_tiny.fb_in1k', 'nasnetalarge.tf_in1k', 'nest_base_jx.goog_in1k', 'nest_small_jx.goog_in1k', 'nest_tiny_jx.goog_in1k', 'nf_regnet_b1.ra2_in1k', 'nf_resnet50.ra2_in1k', 'nfnet_l0.ra2_in1k', 'pit_b_224.in1k', 'pit_b_distilled_224.in1k', 'pit_s_224.in1k', 'pit_s_distilled_224.in1k', 'pit_ti_224.in1k', 'pit_ti_distilled_224.in1k', 'pit_xs_224.in1k', 'pit_xs_distilled_224.in1k', 'pnasnet5large.tf_in1k', 'poolformer_m36.sail_in1k', 'poolformer_m48.sail_in1k', 'poolformer_s12.sail_in1k', 'poolformer_s24.sail_in1k', 'poolformer_s36.sail_in1k', 'poolformerv2_m36.sail_in1k', 'poolformerv2_m48.sail_in1k', 'poolformerv2_s12.sail_in1k', 'poolformerv2_s24.sail_in1k', 'poolformerv2_s36.sail_in1k', 'pvt_v2_b0.in1k', 'pvt_v2_b1.in1k', 'pvt_v2_b2.in1k', 'pvt_v2_b2_li.in1k', 'pvt_v2_b3.in1k', 'pvt_v2_b4.in1k', 'pvt_v2_b5.in1k', 'regnetv_040.ra3_in1k', 'regnetv_064.ra3_in1k', 'regnetx_002.pycls_in1k', 'regnetx_004.pycls_in1k', 'regnetx_004_tv.tv2_in1k', 'regnetx_006.pycls_in1k', 'regnetx_008.pycls_in1k', 'regnetx_008.tv2_in1k', 'regnetx_016.pycls_in1k', 'regnetx_016.tv2_in1k', 'regnetx_032.pycls_in1k', 'regnetx_032.tv2_in1k', 'regnetx_040.pycls_in1k', 'regnetx_064.pycls_in1k', 'regnetx_080.pycls_in1k', 'regnetx_080.tv2_in1k', 'regnetx_120.pycls_in1k', 'regnetx_160.pycls_in1k', 'regnetx_160.tv2_in1k', 'regnetx_320.pycls_in1k', 'regnetx_320.tv2_in1k', 'regnety_002.pycls_in1k', 'regnety_004.pycls_in1k', 'regnety_004.tv2_in1k', 'regnety_006.pycls_in1k', 'regnety_008.pycls_in1k', 'regnety_008_tv.tv2_in1k', 'regnety_016.pycls_in1k', 'regnety_016.tv2_in1k', 'regnety_032.pycls_in1k', 'regnety_032.ra_in1k', 'regnety_032.tv2_in1k', 'regnety_040.pycls_in1k', 'regnety_040.ra3_in1k', 'regnety_064.pycls_in1k', 'regnety_064.ra3_in1k', 'regnety_080.pycls_in1k', 'regnety_080.ra3_in1k', 'regnety_080_tv.tv2_in1k', 'regnety_120.pycls_in1k', 'regnety_120.sw_in12k', 'regnety_120.sw_in12k_ft_in1k', 'regnety_160.deit_in1k', 'regnety_160.lion_in12k_ft_in1k', 'regnety_160.pycls_in1k', 'regnety_160.sw_in12k', 'regnety_160.sw_in12k_ft_in1k', 'regnety_160.swag_ft_in1k', 'regnety_160.swag_lc_in1k', 'regnety_160.tv2_in1k', 'regnety_320.pycls_in1k', 'regnety_320.seer', 'regnety_320.seer_ft_in1k', 'regnety_320.swag_ft_in1k', 'regnety_320.swag_lc_in1k', 'regnety_320.tv2_in1k', 'regnety_640.seer', 'regnety_640.seer_ft_in1k', 'regnety_1280.seer', 'regnety_1280.seer_ft_in1k', 'regnety_1280.swag_ft_in1k', 'regnety_1280.swag_lc_in1k', 'regnety_2560.seer_ft_in1k', 'regnetz_040.ra3_in1k', 'regnetz_040_h.ra3_in1k', 'regnetz_b16.ra3_in1k', 'regnetz_c16.ra3_in1k', 'regnetz_c16_evos.ch_in1k', 'regnetz_d8.ra3_in1k', 'regnetz_d8_evos.ch_in1k', 'regnetz_d32.ra3_in1k', 'regnetz_e8.ra3_in1k', 'repghostnet_050.in1k', 'repghostnet_058.in1k', 'repghostnet_080.in1k', 'repghostnet_100.in1k', 'repghostnet_111.in1k', 'repghostnet_130.in1k', 'repghostnet_150.in1k', 'repghostnet_200.in1k', 'repvgg_a0.rvgg_in1k', 'repvgg_a1.rvgg_in1k', 'repvgg_a2.rvgg_in1k', 'repvgg_b0.rvgg_in1k', 'repvgg_b1.rvgg_in1k', 'repvgg_b1g4.rvgg_in1k', 'repvgg_b2.rvgg_in1k', 'repvgg_b2g4.rvgg_in1k', 'repvgg_b3.rvgg_in1k', 'repvgg_b3g4.rvgg_in1k', 'repvgg_d2se.rvgg_in1k', 'repvit_m1.dist_in1k', 'repvit_m2.dist_in1k', 'repvit_m3.dist_in1k', 'res2net50_14w_8s.in1k', 'res2net50_26w_4s.in1k', 'res2net50_26w_6s.in1k', 'res2net50_26w_8s.in1k', 'res2net50_48w_2s.in1k', 'res2net50d.in1k', 'res2net101_26w_4s.in1k', 'res2net101d.in1k', 'res2next50.in1k', 'resmlp_12_224.fb_dino', 'resmlp_12_224.fb_distilled_in1k', 'resmlp_12_224.fb_in1k', 'resmlp_24_224.fb_dino', 'resmlp_24_224.fb_distilled_in1k', 'resmlp_24_224.fb_in1k', 'resmlp_36_224.fb_distilled_in1k', 'resmlp_36_224.fb_in1k', 'resmlp_big_24_224.fb_distilled_in1k', 'resmlp_big_24_224.fb_in1k', 'resmlp_big_24_224.fb_in22k_ft_in1k', 'resnest14d.gluon_in1k', 'resnest26d.gluon_in1k', 'resnest50d.in1k', 'resnest50d_1s4x24d.in1k', 'resnest50d_4s2x40d.in1k', 'resnest101e.in1k', 'resnest200e.in1k', 'resnest269e.in1k', 'resnet10t.c3_in1k', 'resnet14t.c3_in1k', 'resnet18.a1_in1k', 'resnet18.a2_in1k', 'resnet18.a3_in1k', 'resnet18.fb_ssl_yfcc100m_ft_in1k', 'resnet18.fb_swsl_ig1b_ft_in1k', 'resnet18.gluon_in1k', 'resnet18.tv_in1k', 'resnet18d.ra2_in1k', 'resnet26.bt_in1k', 'resnet26d.bt_in1k', 'resnet26t.ra2_in1k', 'resnet32ts.ra2_in1k', 'resnet33ts.ra2_in1k', 'resnet34.a1_in1k', 'resnet34.a2_in1k', 'resnet34.a3_in1k', 'resnet34.bt_in1k', 'resnet34.gluon_in1k', 'resnet34.tv_in1k', 'resnet34d.ra2_in1k', 'resnet50.a1_in1k', 'resnet50.a1h_in1k', 'resnet50.a2_in1k', 'resnet50.a3_in1k', 'resnet50.am_in1k', 'resnet50.b1k_in1k', 'resnet50.b2k_in1k', 'resnet50.bt_in1k', 'resnet50.c1_in1k', 'resnet50.c2_in1k', 'resnet50.d_in1k', 'resnet50.fb_ssl_yfcc100m_ft_in1k', 'resnet50.fb_swsl_ig1b_ft_in1k', 'resnet50.gluon_in1k', 'resnet50.ra_in1k', 'resnet50.ram_in1k', 'resnet50.tv2_in1k', 'resnet50.tv_in1k', 'resnet50_gn.a1h_in1k', 'resnet50c.gluon_in1k', 'resnet50d.a1_in1k', 'resnet50d.a2_in1k', 'resnet50d.a3_in1k', 'resnet50d.gluon_in1k', 'resnet50d.ra2_in1k', 'resnet50s.gluon_in1k', 'resnet51q.ra2_in1k', 'resnet61q.ra2_in1k', 'resnet101.a1_in1k', 'resnet101.a1h_in1k', 'resnet101.a2_in1k', 'resnet101.a3_in1k', 'resnet101.gluon_in1k', 'resnet101.tv2_in1k', 'resnet101.tv_in1k', 'resnet101c.gluon_in1k', 'resnet101d.gluon_in1k', 'resnet101d.ra2_in1k', 'resnet101s.gluon_in1k', 'resnet152.a1_in1k', 'resnet152.a1h_in1k', 'resnet152.a2_in1k', 'resnet152.a3_in1k', 'resnet152.gluon_in1k', 'resnet152.tv2_in1k', 'resnet152.tv_in1k', 'resnet152c.gluon_in1k', 'resnet152d.gluon_in1k', 'resnet152d.ra2_in1k', 'resnet152s.gluon_in1k', 'resnet200d.ra2_in1k', 'resnetaa50.a1h_in1k', 'resnetaa50d.d_in12k', 'resnetaa50d.sw_in12k', 'resnetaa50d.sw_in12k_ft_in1k', 'resnetaa101d.sw_in12k', 'resnetaa101d.sw_in12k_ft_in1k', 'resnetblur50.bt_in1k', 'resnetrs50.tf_in1k', 'resnetrs101.tf_in1k', 'resnetrs152.tf_in1k', 'resnetrs200.tf_in1k', 'resnetrs270.tf_in1k', 'resnetrs350.tf_in1k', 'resnetrs420.tf_in1k', 'resnetv2_50.a1h_in1k', 'resnetv2_50d_evos.ah_in1k', 'resnetv2_50d_gn.ah_in1k', 'resnetv2_50x1_bit.goog_distilled_in1k', 'resnetv2_50x1_bit.goog_in21k', 'resnetv2_50x1_bit.goog_in21k_ft_in1k', 'resnetv2_50x3_bit.goog_in21k', 'resnetv2_50x3_bit.goog_in21k_ft_in1k', 'resnetv2_101.a1h_in1k', 'resnetv2_101x1_bit.goog_in21k', 'resnetv2_101x1_bit.goog_in21k_ft_in1k', 'resnetv2_101x3_bit.goog_in21k', 'resnetv2_101x3_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_in21k', 'resnetv2_152x2_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384', 'resnetv2_152x4_bit.goog_in21k', 'resnetv2_152x4_bit.goog_in21k_ft_in1k', 'resnext26ts.ra2_in1k', 'resnext50_32x4d.a1_in1k', 'resnext50_32x4d.a1h_in1k', 'resnext50_32x4d.a2_in1k', 'resnext50_32x4d.a3_in1k', 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext50_32x4d.gluon_in1k', 'resnext50_32x4d.ra_in1k', 'resnext50_32x4d.tv2_in1k', 'resnext50_32x4d.tv_in1k', 'resnext50d_32x4d.bt_in1k', 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x4d.gluon_in1k', 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x8d.tv2_in1k', 'resnext101_32x8d.tv_in1k', 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x16d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x32d.fb_wsl_ig1b_ft_in1k', 'resnext101_64x4d.c1_in1k', 'resnext101_64x4d.gluon_in1k', 'resnext101_64x4d.tv_in1k', 'rexnet_100.nav_in1k', 'rexnet_130.nav_in1k', 'rexnet_150.nav_in1k', 'rexnet_200.nav_in1k', 'rexnet_300.nav_in1k', 'rexnetr_200.sw_in12k', 'rexnetr_200.sw_in12k_ft_in1k', 'rexnetr_300.sw_in12k', 'rexnetr_300.sw_in12k_ft_in1k', 'samvit_base_patch16.sa1b', 'samvit_huge_patch16.sa1b', 'samvit_large_patch16.sa1b', 'sebotnet33ts_256.a1h_in1k', 'sehalonet33ts.ra2_in1k', 'selecsls42b.in1k', 'selecsls60.in1k', 'selecsls60b.in1k', 'semnasnet_075.rmsp_in1k', 'semnasnet_100.rmsp_in1k', 'senet154.gluon_in1k', 'sequencer2d_l.in1k', 'sequencer2d_m.in1k', 'sequencer2d_s.in1k', 'seresnet33ts.ra2_in1k', 'seresnet50.a1_in1k', 'seresnet50.a2_in1k', 'seresnet50.a3_in1k', 'seresnet50.ra2_in1k', 'seresnet152d.ra2_in1k', 'seresnext26d_32x4d.bt_in1k', 'seresnext26t_32x4d.bt_in1k', 'seresnext26ts.ch_in1k', 'seresnext50_32x4d.gluon_in1k', 'seresnext50_32x4d.racm_in1k', 'seresnext101_32x4d.gluon_in1k', 'seresnext101_32x8d.ah_in1k', 'seresnext101_64x4d.gluon_in1k', 'seresnext101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.sw_in12k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288', 'seresnextaa201d_32x8d.sw_in12k', 'seresnextaa201d_32x8d.sw_in12k_ft_in1k_384', 'skresnet18.ra_in1k', 'skresnet34.ra_in1k', 'skresnext50_32x4d.ra_in1k', 'spnasnet_100.rmsp_in1k', 'swin_base_patch4_window7_224.ms_in1k', 'swin_base_patch4_window7_224.ms_in22k', 'swin_base_patch4_window7_224.ms_in22k_ft_in1k', 'swin_base_patch4_window12_384.ms_in1k', 'swin_base_patch4_window12_384.ms_in22k', 'swin_base_patch4_window12_384.ms_in22k_ft_in1k', 'swin_large_patch4_window7_224.ms_in22k', 'swin_large_patch4_window7_224.ms_in22k_ft_in1k', 'swin_large_patch4_window12_384.ms_in22k', 'swin_large_patch4_window12_384.ms_in22k_ft_in1k', 'swin_s3_base_224.ms_in1k', 'swin_s3_small_224.ms_in1k', 'swin_s3_tiny_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in22k', 'swin_small_patch4_window7_224.ms_in22k_ft_in1k', 'swin_tiny_patch4_window7_224.ms_in1k', 'swin_tiny_patch4_window7_224.ms_in22k', 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k', 'swinv2_base_window8_256.ms_in1k', 'swinv2_base_window12_192.ms_in22k', 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_base_window16_256.ms_in1k', 'swinv2_cr_small_224.sw_in1k', 'swinv2_cr_small_ns_224.sw_in1k', 'swinv2_cr_tiny_ns_224.sw_in1k', 'swinv2_large_window12_192.ms_in22k', 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_small_window8_256.ms_in1k', 'swinv2_small_window16_256.ms_in1k', 'swinv2_tiny_window8_256.ms_in1k', 'swinv2_tiny_window16_256.ms_in1k', 'tf_efficientnet_b0.aa_in1k', 'tf_efficientnet_b0.ap_in1k', 'tf_efficientnet_b0.in1k', 'tf_efficientnet_b0.ns_jft_in1k', 'tf_efficientnet_b1.aa_in1k', 'tf_efficientnet_b1.ap_in1k', 'tf_efficientnet_b1.in1k', 'tf_efficientnet_b1.ns_jft_in1k', 'tf_efficientnet_b2.aa_in1k', 'tf_efficientnet_b2.ap_in1k', 'tf_efficientnet_b2.in1k', 'tf_efficientnet_b2.ns_jft_in1k', 'tf_efficientnet_b3.aa_in1k', 'tf_efficientnet_b3.ap_in1k', 'tf_efficientnet_b3.in1k', 'tf_efficientnet_b3.ns_jft_in1k', 'tf_efficientnet_b4.aa_in1k', 'tf_efficientnet_b4.ap_in1k', 'tf_efficientnet_b4.in1k', 'tf_efficientnet_b4.ns_jft_in1k', 'tf_efficientnet_b5.aa_in1k', 'tf_efficientnet_b5.ap_in1k', 'tf_efficientnet_b5.in1k', 'tf_efficientnet_b5.ns_jft_in1k', 'tf_efficientnet_b5.ra_in1k', 'tf_efficientnet_b6.aa_in1k', 'tf_efficientnet_b6.ap_in1k', 'tf_efficientnet_b6.ns_jft_in1k', 'tf_efficientnet_b7.aa_in1k', 'tf_efficientnet_b7.ap_in1k', 'tf_efficientnet_b7.ns_jft_in1k', 'tf_efficientnet_b7.ra_in1k', 'tf_efficientnet_b8.ap_in1k', 'tf_efficientnet_b8.ra_in1k', 'tf_efficientnet_cc_b0_4e.in1k', 'tf_efficientnet_cc_b0_8e.in1k', 'tf_efficientnet_cc_b1_8e.in1k', 'tf_efficientnet_el.in1k', 'tf_efficientnet_em.in1k', 'tf_efficientnet_es.in1k', 'tf_efficientnet_l2.ns_jft_in1k', 'tf_efficientnet_l2.ns_jft_in1k_475', 'tf_efficientnet_lite0.in1k', 'tf_efficientnet_lite1.in1k', 'tf_efficientnet_lite2.in1k', 'tf_efficientnet_lite3.in1k', 'tf_efficientnet_lite4.in1k', 'tf_efficientnetv2_b0.in1k', 'tf_efficientnetv2_b1.in1k', 'tf_efficientnetv2_b2.in1k', 'tf_efficientnetv2_b3.in1k', 'tf_efficientnetv2_b3.in21k', 'tf_efficientnetv2_b3.in21k_ft_in1k', 'tf_efficientnetv2_l.in1k', 'tf_efficientnetv2_l.in21k', 'tf_efficientnetv2_l.in21k_ft_in1k', 'tf_efficientnetv2_m.in1k', 'tf_efficientnetv2_m.in21k', 'tf_efficientnetv2_m.in21k_ft_in1k', 'tf_efficientnetv2_s.in1k', 'tf_efficientnetv2_s.in21k', 'tf_efficientnetv2_s.in21k_ft_in1k', 'tf_efficientnetv2_xl.in21k', 'tf_efficientnetv2_xl.in21k_ft_in1k', 'tf_mixnet_l.in1k', 'tf_mixnet_m.in1k', 'tf_mixnet_s.in1k', 'tf_mobilenetv3_large_075.in1k', 'tf_mobilenetv3_large_100.in1k', 'tf_mobilenetv3_large_minimal_100.in1k', 'tf_mobilenetv3_small_075.in1k', 'tf_mobilenetv3_small_100.in1k', 'tf_mobilenetv3_small_minimal_100.in1k', 'tiny_vit_5m_224.dist_in22k', 'tiny_vit_5m_224.dist_in22k_ft_in1k', 'tiny_vit_5m_224.in1k', 'tiny_vit_11m_224.dist_in22k', 'tiny_vit_11m_224.dist_in22k_ft_in1k', 'tiny_vit_11m_224.in1k', 'tiny_vit_21m_224.dist_in22k', 'tiny_vit_21m_224.dist_in22k_ft_in1k', 'tiny_vit_21m_224.in1k', 'tiny_vit_21m_384.dist_in22k_ft_in1k', 'tiny_vit_21m_512.dist_in22k_ft_in1k', 'tinynet_a.in1k', 'tinynet_b.in1k', 'tinynet_c.in1k', 'tinynet_d.in1k', 'tinynet_e.in1k', 'tnt_s_patch16_224', 'tresnet_l.miil_in1k', 'tresnet_l.miil_in1k_448', 'tresnet_m.miil_in1k', 'tresnet_m.miil_in1k_448', 'tresnet_m.miil_in21k', 'tresnet_m.miil_in21k_ft_in1k', 'tresnet_v2_l.miil_in21k', 'tresnet_v2_l.miil_in21k_ft_in1k', 'tresnet_xl.miil_in1k', 'tresnet_xl.miil_in1k_448', 'twins_pcpvt_base.in1k', 'twins_pcpvt_large.in1k', 'twins_pcpvt_small.in1k', 'twins_svt_base.in1k', 'twins_svt_large.in1k', 'twins_svt_small.in1k', 'vgg11.tv_in1k', 'vgg11_bn.tv_in1k', 'vgg13.tv_in1k', 'vgg13_bn.tv_in1k', 'vgg16.tv_in1k', 'vgg16_bn.tv_in1k', 'vgg19.tv_in1k', 'vgg19_bn.tv_in1k', 'visformer_small.in1k', 'visformer_tiny.in1k', 'vit_base_patch8_224.augreg2_in21k_ft_in1k', 'vit_base_patch8_224.augreg_in21k', 'vit_base_patch8_224.augreg_in21k_ft_in1k', 'vit_base_patch8_224.dino', 'vit_base_patch14_dinov2.lvd142m', 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'vit_base_patch16_224.augreg_in1k', 'vit_base_patch16_224.augreg_in21k', 'vit_base_patch16_224.augreg_in21k_ft_in1k', 'vit_base_patch16_224.dino', 'vit_base_patch16_224.mae', 'vit_base_patch16_224.orig_in21k_ft_in1k', 'vit_base_patch16_224.sam_in1k', 'vit_base_patch16_224_miil.in21k', 'vit_base_patch16_224_miil.in21k_ft_in1k', 'vit_base_patch16_384.augreg_in1k', 'vit_base_patch16_384.augreg_in21k_ft_in1k', 'vit_base_patch16_384.orig_in21k_ft_in1k', 'vit_base_patch16_clip_224.datacompxl', 'vit_base_patch16_clip_224.laion2b', 'vit_base_patch16_clip_224.laion2b_ft_in1k', 'vit_base_patch16_clip_224.laion2b_ft_in12k', 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_224.openai', 'vit_base_patch16_clip_224.openai_ft_in1k', 'vit_base_patch16_clip_224.openai_ft_in12k', 'vit_base_patch16_clip_224.openai_ft_in12k_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_384.openai_ft_in1k', 'vit_base_patch16_clip_384.openai_ft_in12k_in1k', 'vit_base_patch16_rpn_224.sw_in1k', 'vit_base_patch32_224.augreg_in1k', 'vit_base_patch32_224.augreg_in21k', 'vit_base_patch32_224.augreg_in21k_ft_in1k', 'vit_base_patch32_224.sam_in1k', 'vit_base_patch32_384.augreg_in1k', 'vit_base_patch32_384.augreg_in21k_ft_in1k', 'vit_base_patch32_clip_224.laion2b', 'vit_base_patch32_clip_224.laion2b_ft_in1k', 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_224.openai', 'vit_base_patch32_clip_224.openai_ft_in1k', 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_384.openai_ft_in12k_in1k', 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k', 'vit_base_r50_s16_224.orig_in21k', 'vit_base_r50_s16_384.orig_in21k_ft_in1k', 'vit_giant_patch14_clip_224.laion2b', 'vit_giant_patch14_dinov2.lvd142m', 'vit_gigantic_patch14_clip_224.laion2b', 'vit_gigantic_patch16_224_ijepa.in22k', 'vit_huge_patch14_224.mae', 'vit_huge_patch14_224.orig_in21k', 'vit_huge_patch14_224_ijepa.in1k', 'vit_huge_patch14_224_ijepa.in22k', 'vit_huge_patch14_clip_224.laion2b', 'vit_huge_patch14_clip_224.laion2b_ft_in1k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_huge_patch16_448_ijepa.in1k', 'vit_large_patch14_clip_224.datacompxl', 'vit_large_patch14_clip_224.laion2b', 'vit_large_patch14_clip_224.laion2b_ft_in1k', 'vit_large_patch14_clip_224.laion2b_ft_in12k', 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_224.openai', 'vit_large_patch14_clip_224.openai_ft_in1k', 'vit_large_patch14_clip_224.openai_ft_in12k', 'vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_336.openai', 'vit_large_patch14_clip_336.openai_ft_in12k_in1k', 'vit_large_patch14_dinov2.lvd142m', 'vit_large_patch16_224.augreg_in21k', 'vit_large_patch16_224.augreg_in21k_ft_in1k', 'vit_large_patch16_224.mae', 'vit_large_patch16_384.augreg_in21k_ft_in1k', 'vit_large_patch32_224.orig_in21k', 'vit_large_patch32_384.orig_in21k_ft_in1k', 'vit_large_r50_s32_224.augreg_in21k', 'vit_large_r50_s32_224.augreg_in21k_ft_in1k', 'vit_large_r50_s32_384.augreg_in21k_ft_in1k', 'vit_medium_patch16_gap_240.sw_in12k', 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k', 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k', 'vit_relpos_base_patch16_224.sw_in1k', 'vit_relpos_base_patch16_clsgap_224.sw_in1k', 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k', 'vit_relpos_medium_patch16_224.sw_in1k', 'vit_relpos_medium_patch16_cls_224.sw_in1k', 'vit_relpos_medium_patch16_rpn_224.sw_in1k', 'vit_relpos_small_patch16_224.sw_in1k', 'vit_small_patch8_224.dino', 'vit_small_patch14_dinov2.lvd142m', 'vit_small_patch16_224.augreg_in1k', 'vit_small_patch16_224.augreg_in21k', 'vit_small_patch16_224.augreg_in21k_ft_in1k', 'vit_small_patch16_224.dino', 'vit_small_patch16_384.augreg_in1k', 'vit_small_patch16_384.augreg_in21k_ft_in1k', 'vit_small_patch32_224.augreg_in21k', 'vit_small_patch32_224.augreg_in21k_ft_in1k', 'vit_small_patch32_384.augreg_in21k_ft_in1k', 'vit_small_r26_s32_224.augreg_in21k', 'vit_small_r26_s32_224.augreg_in21k_ft_in1k', 'vit_small_r26_s32_384.augreg_in21k_ft_in1k', 'vit_srelpos_medium_patch16_224.sw_in1k', 'vit_srelpos_small_patch16_224.sw_in1k', 'vit_tiny_patch16_224.augreg_in21k', 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'vit_tiny_patch16_384.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_224.augreg_in21k', 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k', 'volo_d1_224.sail_in1k', 'volo_d1_384.sail_in1k', 'volo_d2_224.sail_in1k', 'volo_d2_384.sail_in1k', 'volo_d3_224.sail_in1k', 'volo_d3_448.sail_in1k', 'volo_d4_224.sail_in1k', 'volo_d4_448.sail_in1k', 'volo_d5_224.sail_in1k', 'volo_d5_448.sail_in1k', 'volo_d5_512.sail_in1k', 'wide_resnet50_2.racm_in1k', 'wide_resnet50_2.tv2_in1k', 'wide_resnet50_2.tv_in1k', 'wide_resnet101_2.tv2_in1k', 'wide_resnet101_2.tv_in1k', 'xception41.tf_in1k', 'xception41p.ra3_in1k', 'xception65.ra3_in1k', 'xception65.tf_in1k', 'xception65p.ra3_in1k', 'xception71.tf_in1k', 'xcit_large_24_p8_224.fb_dist_in1k', 'xcit_large_24_p8_224.fb_in1k', 'xcit_large_24_p8_384.fb_dist_in1k', 'xcit_large_24_p16_224.fb_dist_in1k', 'xcit_large_24_p16_224.fb_in1k', 'xcit_large_24_p16_384.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_in1k', 'xcit_medium_24_p8_384.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_in1k', 'xcit_medium_24_p16_384.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_in1k', 'xcit_nano_12_p8_384.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_in1k', 'xcit_nano_12_p16_384.fb_dist_in1k', 'xcit_small_12_p8_224.fb_dist_in1k', 'xcit_small_12_p8_224.fb_in1k', 'xcit_small_12_p8_384.fb_dist_in1k', 'xcit_small_12_p16_224.fb_dist_in1k', 'xcit_small_12_p16_224.fb_in1k', 'xcit_small_12_p16_384.fb_dist_in1k', 'xcit_small_24_p8_224.fb_dist_in1k', 'xcit_small_24_p8_224.fb_in1k', 'xcit_small_24_p8_384.fb_dist_in1k', 'xcit_small_24_p16_224.fb_dist_in1k', 'xcit_small_24_p16_224.fb_in1k', 'xcit_small_24_p16_384.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_in1k', 'xcit_tiny_12_p8_384.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_in1k', 'xcit_tiny_12_p16_384.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_in1k', 'xcit_tiny_24_p8_384.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_in1k', 'xcit_tiny_24_p16_384.fb_dist_in1k']\n"
     ]
    }
   ],
   "source": [
    "avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "print(avail_pretrained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a few pre-trained models (Re run this once the epochs are done for the first one, and comment the first one out so that we don't lose our evaluation results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "resnet = timm.create_model('resnet50', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "vgg = timm.create_model('vgg16', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "mobilenet = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "densenet = timm.create_model('densenet121', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "inceptionv4 = timm.create_model('inception_v4', pretrained=True, num_classes=2, drop_rate=0.2).to(device)\n",
    "# coca = timm.create_model('coca_s', pretrained=True, num_classes=2, drop_rate=0.2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Path Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Demented', 'Non-Demented']\n",
      "['Demented', 'Non-Demented']\n",
      "['Demented', 'Non-Demented']\n",
      "['Demented', 'Non-Demented']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('Alzheimer_s Dataset_binary\\\\src')) # Preliminary Image Processing\\Alzheimer_s Dataset_binary\\src\n",
    "print(os.listdir('Alzheimer_s Dataset_binary\\\\train'))\n",
    "print(os.listdir('Alzheimer_s Dataset_binary\\\\test'))\n",
    "print(os.listdir('Alzheimer_s Dataset_binary\\\\val'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source folder containing subfolders for each class\n",
    "source_folder = 'Alzheimer_s Dataset_binary\\\\src'\n",
    "train_folder = 'Alzheimer_s Dataset_binary\\\\train'\n",
    "test_folder = 'Alzheimer_s Dataset_binary\\\\test'\n",
    "val_folder = 'Alzheimer_s Dataset_binary\\\\val'\n",
    "\n",
    "# Define the ratios for train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "val_ratio = 0.15\n",
    "\n",
    "# Create destination folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "# List subfolders (classes) in the source folder\n",
    "classes = os.listdir(source_folder)\n",
    "\n",
    "# Loop through each class\n",
    "for class_name in classes:\n",
    "    class_source_folder = os.path.join(source_folder, class_name)\n",
    "    class_train_folder = os.path.join(train_folder, class_name)\n",
    "    class_test_folder = os.path.join(test_folder, class_name)\n",
    "    class_val_folder = os.path.join(val_folder, class_name)\n",
    "\n",
    "    # List image files in the class source folder\n",
    "    image_files = os.listdir(class_source_folder)\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Calculate the number of images for each set based on the defined ratios\n",
    "    total_images = len(image_files)\n",
    "    train_split = int(total_images * train_ratio)\n",
    "    test_split = int(total_images * test_ratio)\n",
    "\n",
    "    # Copy image files to their respective folders for train, test, and validation\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        source_path = os.path.join(class_source_folder, image_file)\n",
    "        if i < train_split:\n",
    "            destination_folder = class_train_folder\n",
    "        elif i < train_split + test_split:\n",
    "            destination_folder = class_test_folder\n",
    "        else:\n",
    "            destination_folder = class_val_folder\n",
    "\n",
    "        destination_path = os.path.join(destination_folder, image_file)\n",
    "        shutil.copyfile(source_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((176, 208)),  # Resize images to a common size (adjust as needed)\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/train', transform=transform)\n",
    "test_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/test', transform=transform)\n",
    "val_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/val', transform=transform)\n",
    "\n",
    "batch_size = 26  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(len(val_loader))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1 with loss 0.5972965955734253, validation loss 0.662198635439078\n",
      "Completed epoch 2 with loss 0.7139240503311157, validation loss 0.6726262635654874\n",
      "Completed epoch 3 with loss 0.703946053981781, validation loss 0.6943394367893537\n",
      "Completed epoch 4 with loss 0.6387301683425903, validation loss 0.6387741545008289\n",
      "Completed epoch 5 with loss 0.6101664304733276, validation loss 0.5739335707492299\n",
      "Completed epoch 6 with loss 0.4487069547176361, validation loss 0.5758718889620569\n",
      "Completed epoch 7 with loss 0.5640740990638733, validation loss 0.562455779976315\n",
      "Completed epoch 8 with loss 0.49725598096847534, validation loss 0.546795973347293\n",
      "Completed epoch 9 with loss 0.42574071884155273, validation loss 0.532713406615787\n",
      "Completed epoch 10 with loss 0.2875150442123413, validation loss 0.562677327129576\n",
      "Completed epoch 11 with loss 0.43426617980003357, validation loss 0.5070842413438691\n",
      "Completed epoch 12 with loss 0.5006571412086487, validation loss 0.6812141231364675\n",
      "Completed epoch 13 with loss 0.35687965154647827, validation loss 0.5063385226660304\n",
      "Completed epoch 14 with loss 0.5218497514724731, validation loss 0.5114595687223805\n",
      "Completed epoch 15 with loss 0.5038424134254456, validation loss 0.5659933721439706\n",
      "Completed epoch 16 with loss 0.228412464261055, validation loss 0.6727111354056332\n",
      "Completed epoch 17 with loss 0.3867318630218506, validation loss 0.5000854416026009\n",
      "Completed epoch 18 with loss 0.3293747901916504, validation loss 0.5467312762306796\n",
      "Completed epoch 19 with loss 0.3918297588825226, validation loss 0.9164690154397653\n",
      "Completed epoch 20 with loss 0.4379149377346039, validation loss 0.4887545746233728\n",
      "Completed epoch 21 with loss 0.24892136454582214, validation loss 0.8467136657693319\n",
      "Completed epoch 22 with loss 0.31902360916137695, validation loss 0.5359909567568037\n",
      "Completed epoch 23 with loss 0.27149516344070435, validation loss 0.7279119525725642\n",
      "Completed epoch 24 with loss 0.15210404992103577, validation loss 0.5899824635643098\n",
      "Completed epoch 25 with loss 0.037894897162914276, validation loss 0.5643793423142698\n",
      "Early stopping triggered\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 77.34%\n"
     ]
    }
   ],
   "source": [
    "loss_value = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "resnet_opt = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  \n",
    "early_stopping = False  \n",
    "best_loss = float('inf') \n",
    "patience = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()  \n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        resnet_opt.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = loss_value(outputs, labels)\n",
    "        loss.backward()\n",
    "        resnet_opt.step()\n",
    "    \n",
    "    # Validation\n",
    "    resnet.eval()  \n",
    "    val_loss = 0.0\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        val_outputs = resnet(val_inputs)\n",
    "        val_loss += loss_value(val_outputs, val_labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Completed epoch {epoch + 1} with loss {loss.item()}, validation loss {val_loss}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        \n",
    "        # If validation loss hasn't improved for 'patience' epochs, stop early\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        break\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "resnet.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1 with loss 0.7225592732429504, validation loss 0.5643793423142698\n",
      "Completed epoch 2 with loss 0.40115609765052795, validation loss 0.5643793423142698\n",
      "Completed epoch 3 with loss 0.17567065358161926, validation loss 0.5643793423142698\n",
      "Completed epoch 4 with loss 0.18134020268917084, validation loss 0.5643793423142698\n",
      "Completed epoch 5 with loss 0.1701154261827469, validation loss 0.5643793423142698\n",
      "Completed epoch 6 with loss 0.20956775546073914, validation loss 0.5643793423142698\n",
      "Early stopping triggered\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 93.25%\n"
     ]
    }
   ],
   "source": [
    "loss_value = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "vgg_opt = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  \n",
    "early_stopping = False  \n",
    "best_loss = float('inf') \n",
    "patience = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vgg.train()  \n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        vgg_opt.zero_grad()\n",
    "        outputs = vgg(inputs)\n",
    "        loss = loss_value(outputs, labels)\n",
    "        loss.backward()\n",
    "        vgg_opt.step()\n",
    "    \n",
    "    # Validation\n",
    "    vgg.eval()  \n",
    "    val_loss = 0.0\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        val_outputs = resnet(val_inputs)\n",
    "        val_loss += loss_value(val_outputs, val_labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Completed epoch {epoch + 1} with loss {loss.item()}, validation loss {val_loss}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        \n",
    "        # If validation loss hasn't improved for 'patience' epochs, stop early\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        break\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "vgg.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = vgg(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1 with loss 0.7445648312568665, validation loss 0.6043489978959163\n",
      "Completed epoch 2 with loss 0.19247862696647644, validation loss 0.5476116006676521\n",
      "Completed epoch 3 with loss 0.24822600185871124, validation loss 0.5780754101116246\n",
      "Completed epoch 4 with loss 0.023607904091477394, validation loss 0.25335530386332217\n",
      "Completed epoch 5 with loss 0.022079359740018845, validation loss 0.19679576785468575\n",
      "Completed epoch 6 with loss 0.07140202820301056, validation loss 0.21156224173804125\n",
      "Completed epoch 7 with loss 0.006242365576326847, validation loss 0.28764441644175853\n",
      "Completed epoch 8 with loss 0.00026606657775118947, validation loss 0.15315801704764453\n",
      "Completed epoch 9 with loss 0.0002453562628943473, validation loss 0.19076891623707423\n",
      "Completed epoch 10 with loss 5.692768536391668e-05, validation loss 0.1418735117838676\n",
      "Completed epoch 11 with loss 0.1272803544998169, validation loss 0.25029791052803474\n",
      "Completed epoch 12 with loss 0.27927345037460327, validation loss 0.15894512526372434\n",
      "Completed epoch 13 with loss 0.0004837018495891243, validation loss 0.18516681901706356\n",
      "Completed epoch 14 with loss 0.00015517501742579043, validation loss 0.18588017663015913\n",
      "Completed epoch 15 with loss 0.003606501966714859, validation loss 0.14806717493757382\n",
      "Early stopping triggered\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 97.39%\n"
     ]
    }
   ],
   "source": [
    "loss_value = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mobilenet_opt = optim.SGD(mobilenet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  \n",
    "early_stopping = False  \n",
    "best_loss = float('inf') \n",
    "patience = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mobilenet.train()  \n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        mobilenet_opt.zero_grad()\n",
    "        outputs = mobilenet(inputs)\n",
    "        loss = loss_value(outputs, labels)\n",
    "        loss.backward()\n",
    "        mobilenet_opt.step()\n",
    "    \n",
    "    # Validation\n",
    "    mobilenet.eval()  \n",
    "    val_loss = 0.0\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        val_outputs = mobilenet(val_inputs)\n",
    "        val_loss += loss_value(val_outputs, val_labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Completed epoch {epoch + 1} with loss {loss.item()}, validation loss {val_loss}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        \n",
    "        # If validation loss hasn't improved for 'patience' epochs, stop early\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        break\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "mobilenet.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobilenet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1 with loss 0.4529228210449219, validation loss 0.5130811159809431\n",
      "Completed epoch 2 with loss 0.22813192009925842, validation loss 0.3457600958645344\n",
      "Completed epoch 3 with loss 0.20799240469932556, validation loss 0.33085070876404643\n",
      "Completed epoch 4 with loss 0.03374020382761955, validation loss 0.16575739196398193\n",
      "Completed epoch 5 with loss 0.04299529641866684, validation loss 0.10417538092264698\n",
      "Completed epoch 6 with loss 0.21213951706886292, validation loss 0.12881737153252792\n",
      "Completed epoch 7 with loss 0.024899544194340706, validation loss 0.13456355940757525\n",
      "Completed epoch 8 with loss 0.01435030810534954, validation loss 0.09219071206300417\n",
      "Completed epoch 9 with loss 0.007303235586732626, validation loss 0.10569769330969495\n",
      "Completed epoch 10 with loss 0.002662542974576354, validation loss 0.1526929394183551\n",
      "Completed epoch 11 with loss 0.005069334991276264, validation loss 0.09652588513886763\n",
      "Completed epoch 12 with loss 0.05151045322418213, validation loss 0.10596863727550954\n",
      "Completed epoch 13 with loss 0.075630322098732, validation loss 0.0637075202151512\n",
      "Completed epoch 14 with loss 0.006485641002655029, validation loss 0.0809724957459063\n",
      "Completed epoch 15 with loss 0.03752216696739197, validation loss 0.0700025214755442\n",
      "Completed epoch 16 with loss 0.003769515547901392, validation loss 0.10003145056220496\n",
      "Completed epoch 17 with loss 0.005165324546396732, validation loss 0.05356546309404722\n",
      "Completed epoch 18 with loss 0.00025455228751525283, validation loss 0.042835288537591824\n",
      "Completed epoch 19 with loss 0.0023822186049073935, validation loss 0.08374651699048424\n",
      "Completed epoch 20 with loss 0.005849041510373354, validation loss 0.06056493875156674\n",
      "Completed epoch 21 with loss 0.000647007196675986, validation loss 0.04262167145983161\n",
      "Completed epoch 22 with loss 0.000321664527291432, validation loss 0.04267855119137999\n",
      "Completed epoch 23 with loss 0.11250193417072296, validation loss 0.04010933953456212\n",
      "Completed epoch 24 with loss 0.0007405976066365838, validation loss 0.04446334121732636\n",
      "Completed epoch 25 with loss 0.0006234439206309617, validation loss 0.040133437486575635\n",
      "Completed epoch 26 with loss 0.0002838847867678851, validation loss 0.0421072252029262\n",
      "Completed epoch 27 with loss 0.0003307572042103857, validation loss 0.03977091560833893\n",
      "Completed epoch 28 with loss 0.00029259739676490426, validation loss 0.03793963322419182\n",
      "Completed epoch 29 with loss 6.749050226062536e-05, validation loss 0.047948020384562064\n",
      "Completed epoch 30 with loss 1.1205455848539714e-05, validation loss 0.04028560123636756\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 97.82%\n"
     ]
    }
   ],
   "source": [
    "loss_value = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "densenet_opt = optim.SGD(densenet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  \n",
    "early_stopping = False  \n",
    "best_loss = float('inf') \n",
    "patience = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    densenet.train()  \n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        densenet_opt.zero_grad()\n",
    "        outputs = densenet(inputs)\n",
    "        loss = loss_value(outputs, labels)\n",
    "        loss.backward()\n",
    "        densenet_opt.step()\n",
    "    \n",
    "    # Validation\n",
    "    densenet.eval()  \n",
    "    val_loss = 0.0\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        val_outputs = densenet(val_inputs)\n",
    "        val_loss += loss_value(val_outputs, val_labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Completed epoch {epoch + 1} with loss {loss.item()}, validation loss {val_loss}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        \n",
    "        # If validation loss hasn't improved for 'patience' epochs, stop early\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        break\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "densenet.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = densenet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inceptionv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1 with loss 0.5729765295982361, validation loss 0.5034215864208009\n",
      "Completed epoch 2 with loss 0.33148831129074097, validation loss 0.4140690445072121\n",
      "Completed epoch 3 with loss 0.11480871587991714, validation loss 0.2912245374172926\n",
      "Completed epoch 4 with loss 0.01429636962711811, validation loss 0.13900020946231154\n",
      "Completed epoch 5 with loss 0.01847616583108902, validation loss 0.1619707698571599\n",
      "Completed epoch 6 with loss 0.0319196954369545, validation loss 0.16890166338352072\n",
      "Completed epoch 7 with loss 0.004581802524626255, validation loss 0.17587602535624886\n",
      "Completed epoch 8 with loss 0.04067014530301094, validation loss 0.13350597533604336\n",
      "Completed epoch 9 with loss 0.003873853012919426, validation loss 0.1274900399795216\n",
      "Completed epoch 10 with loss 0.048844993114471436, validation loss 0.1319852301902655\n",
      "Completed epoch 11 with loss 0.0027840337716042995, validation loss 0.09555844770834988\n",
      "Completed epoch 12 with loss 0.0019335746765136719, validation loss 0.0881821113677385\n",
      "Completed epoch 13 with loss 0.001290956512093544, validation loss 0.11219274394453452\n",
      "Completed epoch 14 with loss 0.012437677942216396, validation loss 0.10200267706790732\n",
      "Completed epoch 15 with loss 0.00013971669250167906, validation loss 0.09682284978852193\n",
      "Completed epoch 16 with loss 0.0020510682370513678, validation loss 0.12028981606353126\n",
      "Completed epoch 17 with loss 0.0016727533657103777, validation loss 0.08742297313680562\n",
      "Completed epoch 18 with loss 0.00021685597312171012, validation loss 0.07797883633126428\n",
      "Completed epoch 19 with loss 0.018335891887545586, validation loss 0.06650438524633905\n",
      "Completed epoch 20 with loss 0.0012254135217517614, validation loss 0.08373085311743328\n",
      "Completed epoch 21 with loss 0.007219097577035427, validation loss 0.08287304664392853\n",
      "Completed epoch 22 with loss 0.009136855602264404, validation loss 0.09244214954701925\n",
      "Completed epoch 23 with loss 0.004415588919073343, validation loss 0.09077978226701412\n",
      "Completed epoch 24 with loss 0.00013467937242239714, validation loss 0.08435123089374327\n",
      "Early stopping triggered\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 97.17%\n"
     ]
    }
   ],
   "source": [
    "loss_value = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "inceptionv4_opt = optim.SGD(inceptionv4.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  \n",
    "early_stopping = False  \n",
    "best_loss = float('inf') \n",
    "patience = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inceptionv4.train()  \n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inceptionv4_opt.zero_grad()\n",
    "        outputs = inceptionv4(inputs)\n",
    "        loss = loss_value(outputs, labels)\n",
    "        loss.backward()\n",
    "        inceptionv4_opt.step()\n",
    "    \n",
    "    # Validation\n",
    "    inceptionv4.eval()  \n",
    "    val_loss = 0.0\n",
    "    for val_inputs, val_labels in val_loader:\n",
    "        val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "        val_outputs = inceptionv4(val_inputs)\n",
    "        val_loss += loss_value(val_outputs, val_labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Completed epoch {epoch + 1} with loss {loss.item()}, validation loss {val_loss}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        \n",
    "        # If validation loss hasn't improved for 'patience' epochs, stop early\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            early_stopping = True\n",
    "            break\n",
    "    \n",
    "    if early_stopping:\n",
    "        break\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "inceptionv4.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = inceptionv4(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet, 'resnet50.pth')\n",
    "torch.save(vgg, 'vgg16.pth')\n",
    "torch.save(mobilenet, 'mobilenetv3.pth')\n",
    "torch.save(densenet, 'densenet121.pth')\n",
    "torch.save(inceptionv4, 'inception_v4.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New split train-test-validation for overfitting check, we will skip training and only check test values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet2 = torch.load('densenet121.pth')\n",
    "inception2 = torch.load('inception_v4.pth')\n",
    "mobilenet2 = torch.load('mobilenetv3.pth')\n",
    "resnet2 = torch.load('resnet50.pth')\n",
    "vgg2 = torch.load('vgg16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train-test-splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source folder containing subfolders for each class\n",
    "source_folder = 'Alzheimer_s Dataset_binary\\\\src'\n",
    "train_folder = 'Alzheimer_s Dataset_binary\\\\ctrain'\n",
    "test_folder = 'Alzheimer_s Dataset_binary\\\\ctest'\n",
    "val_folder = 'Alzheimer_s Dataset_binary\\\\cval'\n",
    "\n",
    "# Define the ratios for train, test, and validation sets\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.15\n",
    "val_ratio = 0.10\n",
    "\n",
    "# Create destination folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "# List subfolders (classes) in the source folder\n",
    "classes = os.listdir(source_folder)\n",
    "\n",
    "# Loop through each class\n",
    "for class_name in classes:\n",
    "    class_source_folder = os.path.join(source_folder, class_name)\n",
    "    class_train_folder = os.path.join(train_folder, class_name)\n",
    "    class_test_folder = os.path.join(test_folder, class_name)\n",
    "    class_val_folder = os.path.join(val_folder, class_name)\n",
    "\n",
    "    # List image files in the class source folder\n",
    "    image_files = os.listdir(class_source_folder)\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Calculate the number of images for each set based on the defined ratios\n",
    "    total_images = len(image_files)\n",
    "    train_split = int(total_images * train_ratio)\n",
    "    test_split = int(total_images * test_ratio)\n",
    "\n",
    "    # Copy image files to their respective folders for train, test, and validation\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        source_path = os.path.join(class_source_folder, image_file)\n",
    "        if i < train_split:\n",
    "            destination_folder = class_train_folder\n",
    "        elif i < train_split + test_split:\n",
    "            destination_folder = class_test_folder\n",
    "        else:\n",
    "            destination_folder = class_val_folder\n",
    "\n",
    "        destination_path = os.path.join(destination_folder, image_file)\n",
    "        shutil.copyfile(source_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the train-test-splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "36\n",
      "24\n",
      "4590\n",
      "918\n",
      "613\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((176, 208)),  # Resize images to a common size (adjust as needed)\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/ctrain', transform=transform)\n",
    "test_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/ctest', transform=transform)\n",
    "val_dataset = ImageFolder(root='Alzheimer_s Dataset_binary/cval', transform=transform)\n",
    "\n",
    "batch_size = 26  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(len(val_loader))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "\n",
      "Accuracy on the test dataset: 99.56%\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "densenet2.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = densenet2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print('\\n\\nAccuracy on the test dataset: {:.2f}%'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dementia_Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
